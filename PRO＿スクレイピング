from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import os
import re  # 正規表現用ライブラリ
import time  # スクロール待機用

# WebDriverのパスを設定
driver_path = 'C:\\Users\\USER\\Desktop\\chromedriver.exe'

# サービスオブジェクトを作成してWebDriverに渡す
service = Service(driver_path)
driver = webdriver.Chrome(service=service)

# 指定されたURLにアクセス
url = 'https://www.hanacupid.or.jp/stores/results?pref=01&city=203'
driver.get(url)

# ページの読み込みを待機
try:
    # 'data' クラスの <div> が見つかるまで最大10秒待機
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'data')))
    print("Page loaded successfully.")
except:
    print("Error: The 'data' div could not be found.")
    driver.quit()
    exit()

# スクロールしてページ全体のデータを読み込む関数
def scroll_to_bottom(driver):
    """ページを下までスクロールする関数"""
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)  # スクロール後にデータ読み込みの時間を待つ
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

# スクロールしてページを下まで移動
scroll_to_bottom(driver)

# BeautifulSoupを使用してページソースを解析
soup = BeautifulSoup(driver.page_source, 'html.parser')

# 店名を取得
store_name_tag = soup.find('h2', class_='storeName')
store_name = store_name_tag.text.strip() if store_name_tag else '店名が見つかりませんでした'

# 定休日情報を取得 (CSSセレクタを使用)
holiday_element = soup.select_one('dt:contains("定休日") + dd')
holiday_info = holiday_element.text.strip() if holiday_element else '定休日情報が見つかりませんでした'

# 営業時間と祝日情報を取得する関数
def get_business_hours_and_holidays(soup):
    """営業時間と祝日情報を取得する関数"""
    # 営業時間のCSSセレクタやXPath
    business_hours_selectors = [
        "dt:contains('営業時間') + dd",
        "#wrap > main > article > div > div > div.data > dl > dd:nth-child(9)",
        "div.data dl > dd:nth-of-type(5)",
        "div.data dd:nth-of-type(5)"  # ここはサンプル
    ]
    
    # 祝日のCSSセレクタやXPath
    holiday_selectors = [
        "dt:contains('祝日') + dd",
        "#wrap > main > article > div > div > div.data > dl > dd:nth-child(10)",
        "div.data dl > dd:nth-of-type(6)",
        "div.data dd:nth-of-type(6)"  # ここはサンプル
    ]
    
    # 営業時間情報の取得
    for selector in business_hours_selectors:
        business_hours_element = soup.select_one(selector)
        if business_hours_element:
            business_hours = business_hours_element.text.strip()
            break
    else:
        business_hours = "営業時間情報が見つかりませんでした"

    # 祝日情報の取得
    for selector in holiday_selectors:
        holiday_element = soup.select_one(selector)
        if holiday_element:
            holiday_info = holiday_element.text.strip()
            break
    else:
        holiday_info = "祝日情報が見つかりませんでした"
    
    return business_hours, holiday_info

# 住所から郵便番号を削除する関数
def remove_postal_code(address):
    """住所から郵便番号を削除する関数"""
    # 郵便番号の正規表現パターン
    postal_code_pattern = r'〒\d{3}-\d{4}'
    return re.sub(postal_code_pattern, '', address).strip()

# 'data' クラスの <div> を取得
data_div = soup.find('div', class_='data')

# データ格納用リスト
data_list = []

if data_div is not None:
    # <dt> と <dd> のペアを取得してリストに格納
    for dt, dd in zip(data_div.find_all('dt'), data_div.find_all('dd')):
        item_name = dt.text.strip()
        item_content = dd.text.strip()

        # 「配達エリア」の文字列を削除
        if "配達エリア" in item_name:
            item_name = item_name.replace("配達エリア", "").strip()

        # データ辞書を作成
        data_dict = {
            '店名': store_name,
            '住所': '',
            '電話番号': '',
            '定休日': holiday_info,
            '営業時間': '',
            '祝日': ''
        }

        # 辞書に対応するデータを格納
        if "住所" in item_name:
            data_dict['住所'] = remove_postal_code(item_content)  # 郵便番号を削除
        elif "電話番号" in item_name:
            data_dict['電話番号'] = item_content
        elif "営業時間" in item_name:
            data_dict['営業時間'] = item_content

        # 営業時間と祝日情報を取得
        business_hours, holiday_info = get_business_hours_and_holidays(soup)
        data_dict['営業時間'] = business_hours
        data_dict['祝日'] = holiday_info

        data_list.append(data_dict)

# 出力用データフレームを作成
df = pd.DataFrame(data_list)

# 出力ファイルパスをユーザーのドキュメントフォルダに設定
output_directory = os.path.join(os.path.expanduser('~'), 'Documents')
output_file_path = os.path.join(output_directory, 'ohanayasan_result_info.xlsx')

# Excelに出力
with pd.ExcelWriter(output_file_path, engine='openpyxl') as writer:
    # ヘッダー情報を設定
    header = ['店名', '住所', '電話番号', '定休日', '営業時間（※平日のみ）', '祝日']
    df.to_excel(writer, index=False, header=header, startrow=0, startcol=0)

print(f"Excelファイル '{output_file_path}' に保存されました！")

# ウェブドライバーを終了
driver.quit()
