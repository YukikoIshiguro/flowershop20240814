from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import os

# WebDriverのパスを設定
driver_path = 'C:\\Users\\USER\\Desktop\\chromedriver.exe'

# サービスオブジェクトを作成してWebDriverに渡す
service = Service(driver_path)
driver = webdriver.Chrome(service=service)

# 指定されたURLにアクセス
url = 'https://www.hanacupid.or.jp/stores/details/03101'
driver.get(url)

# ページの読み込みを待機
try:
    # 'data' クラスの <div> が見つかるまで最大10秒待機
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'data')))
    print("Page loaded successfully.")
except:
    print("Error: The 'data' div could not be found.")
    driver.quit()
    exit()

# BeautifulSoupを使用してページソースを解析
soup = BeautifulSoup(driver.page_source, 'html.parser')

# 'data' クラスの <div> を取得
data_div = soup.find('div', class_='data')

if data_div is not None:
    # データを格納するリストを作成
    data = []

    # <dt> と <dd> のペアを取得してデータリストに追加
    for dt, dd in zip(data_div.find_all('dt'), data_div.find_all('dd')):
        print(f"{dt.text.strip()}: {dd.text.strip()}")
        data.append({'項目': dt.text.strip(), '内容': dd.text.strip()})
    
    # データフレームを作成
    df = pd.DataFrame(data)

    # 出力ファイルパスをユーザーのドキュメントフォルダに設定
    output_directory = os.path.join(os.path.expanduser('~'), 'Documents')
    output_file_path = os.path.join(output_directory, 'ohanayasan_result_info.xlsx')

    # データフレームをExcelファイルに書き出す
    df.to_excel(output_file_path, index=False, engine='openpyxl')

    print(f"Excelファイル '{output_file_path}' に保存されました！")

else:
    print("Error: 'data' div was not found in the page source.")

# ウェブドライバーを終了
driver.quit()
#2024_08_14お花屋さん＿配達エリアはいらない＿店名がとれてない問題
